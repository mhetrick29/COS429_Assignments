{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hog36 import hog36\n",
    "import math \n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from glob import glob\n",
    "from logistic_prob import logistic_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matt Hetrick\\Desktop\\COS429\\Assignment 2\\a2_part3_starter\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Matt Hetrick\\Desktop\\COS429\\Assignment 2\\a2_part3_starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import math \n",
    "from hog36 import hog36\n",
    "\n",
    "\n",
    "def get_training_data(n, orientations, wrap180):\n",
    "    \"\"\"Reads in examples of faces and nonfaces, and builds a matrix of HoG\n",
    "       descriptors, ready to pass in to logistic_fit\n",
    "\n",
    "    Args:\n",
    "        n: nu2mber of face and nonface training examples (n of each)\n",
    "        orientations: the number of HoG gradient orientations to use\n",
    "        wrap180: if true, the HoG orientations cover 180 degrees, else 360\n",
    "\n",
    "    Returns:\n",
    "        descriptors: matrix of descriptors for all 2*n training examples, where\n",
    "                     each row contains the HoG descriptor for one face or nonface\n",
    "        classes: vector indicating whether each example is a face (1) or nonface (0)\n",
    "    \"\"\"\n",
    "    training_faces_dir = 'face_data/training_faces'\n",
    "    training_nonfaces_dir = 'face_data/training_nonfaces'\n",
    "    hog_input_size = 36\n",
    "    hog_descriptor_size = 100 * orientations\n",
    "\n",
    "    # Get the names of the first n training faces\n",
    "    face_filenames = sorted(glob(os.path.join(training_faces_dir, '*.jpg')))\n",
    "    num_face_filenames = len(face_filenames)\n",
    "    if num_face_filenames > n:\n",
    "        face_filenames = face_filenames[:n]\n",
    "    elif num_face_filenames < n:\n",
    "        n = num_face_filenames\n",
    "\n",
    "    # Initialize descriptors, classes\n",
    "    descriptors = np.zeros([2 * n, hog_descriptor_size + 1])\n",
    "    classes = np.zeros([2 * n])\n",
    "\n",
    "    # Loop over faces\n",
    "    for i in range(n):\n",
    "        # Read the next face file\n",
    "        face = cv2.imread(face_filenames[i], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Compute HoG descriptor\n",
    "        face_descriptor = hog36(face, orientations, wrap180)\n",
    "\n",
    "        # Fill in descriptors and classes\n",
    "        descriptors[i, 0] = 1\n",
    "        descriptors[i, 1:] = face_descriptor\n",
    "        classes[i] = 1\n",
    "\n",
    "    # Get the names of the nonfaces\n",
    "    nonface_filenames = sorted(glob(os.path.join(training_nonfaces_dir, '*.jpg')))\n",
    "    num_nonface_filenames = len(nonface_filenames)\n",
    "\n",
    "    # Loop over all nonface samples we want\n",
    "    for i in range(n, 2 * n):\n",
    "        # Read a random nonface file\n",
    "        j = random.randint(0, num_nonface_filenames - 1)\n",
    "        nonface = cv2.imread(nonface_filenames[j], cv2.IMREAD_GRAYSCALE)\n",
    " \n",
    "        # random numbers for random cropping :: crop set up \n",
    "        height = nonface.shape[0]\n",
    "        width = nonface.shape[1]   \n",
    "        split = hog_input_size/2\n",
    "        maxEdge = min(height-1,width-1)\n",
    "        # put -2 so we dont pick a corner pixel for convenience of boundary cases\n",
    "        # need a size of at least 36 so that resize doesnt fail so location cannot be under 18 on either side\n",
    "        location = random.randint(split, maxEdge-split)\n",
    "        # loc+size <= min(h,w) and loc-size >= 0. Solve to get loc-min(h,w) <= size <= loc\n",
    "        size = random.randint(split, min(math.fabs(0-location), maxEdge - location))\n",
    "    \n",
    "        # Crop out a random square at least hog_input_size\n",
    "        # random size square \n",
    "        crop = nonface[location-size:location+size,location-size:location+size] \n",
    "\n",
    "        # Resize to be the right size\n",
    "        crop = cv2.resize(crop, (hog_input_size, hog_input_size))\n",
    "        # Compute HoG descriptor\n",
    "        non_face_descriptor = hog36(crop, orientations, wrap180)\n",
    "\n",
    "        # Fill in descriptors and classes\n",
    "        descriptors[i, 0] = 1\n",
    "        descriptors[i, 1:] = non_face_descriptor\n",
    "        classes[i] = 0\n",
    "\n",
    "    return descriptors, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from get_training_data import get_training_data\n",
    "#from get_testing_data import get_testing_data\n",
    "from logistic_prob import logistic_prob\n",
    "from logistic_fit import logistic_fit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def test_face_classifier(ntrain, ntest, orientations, wrap180):\n",
    "    \"\"\"Train and test a face classifier.\n",
    "\n",
    "    Args:\n",
    "        ntrain: number of face and nonface training examples (ntrain of each)\n",
    "        ntest: number of face and nonface testing examples (ntest of each)\n",
    "        orientations: the number of HoG gradient orientations to use\n",
    "        wrap180: if true, the HoG orientations cover 180 degrees, else 360\n",
    "    \"\"\"\n",
    "    # Get some training data\n",
    "    descriptors, classes = get_training_data(ntrain, orientations, wrap180)\n",
    "\n",
    "    # Train a classifier\n",
    "    params = logistic_fit(descriptors, classes, 0.001)\n",
    "\n",
    "    # Evaluate the classifier on the training data\n",
    "    predicted = logistic_prob(descriptors, params)\n",
    "    plot_errors(predicted, classes, 'Performance on training set for varying threshold', 1)\n",
    "\n",
    "    # Get some test data\n",
    "    tdescriptors, tclasses = get_testing_data(ntest, orientations, wrap180)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    tpredicted = logistic_prob(tdescriptors, params)\n",
    "    plot_errors(tpredicted, tclasses, 'Performance on test set for varying threshold', 2)\n",
    "\n",
    "\n",
    "def plot_errors(predicted, classes, name, num):\n",
    "    \"\"\"Plot a log/log graph of miss rate (false negatives) vs false positives\n",
    "       for a variety of thresholds on probability.\n",
    "\n",
    "    Args:\n",
    "        predicted: probabilities that the class is 1\n",
    "        classes: ground-truth class labels (0/1)\n",
    "        name: name of the figure\n",
    "        num: number of the figure\n",
    "    \"\"\"\n",
    "    nthresh = 99\n",
    "    npts = predicted.shape[0]\n",
    "\n",
    "    falsepos = np.zeros([nthresh])\n",
    "    falseneg = np.zeros([nthresh])\n",
    "\n",
    "    stepsize = 1. / (nthresh + 1)\n",
    "    for i in range(nthresh):\n",
    "        thresh = (i + 1) * stepsize\n",
    "        falsepos[i] = np.sum(np.logical_and(predicted >= thresh, classes == 0)) / npts\n",
    "        falseneg[i] = np.sum(np.logical_and(predicted < thresh, classes == 1)) / npts\n",
    "\n",
    "    limit = 1e-4\n",
    "    plt.figure(num)\n",
    "    plt.title(name)\n",
    "    plt.loglog(np.maximum(falsepos, limit), np.maximum(falseneg, limit))\n",
    "    plt.axis([limit, 1, limit, 1])\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('False negative rate')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    np.savez('face_classifier.npz', params = params, orientations=orientations, wrap180=wrap180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faces_single_scale(img, stride, thresh, params, orientations, wrap180):\n",
    "    \"\"\"Find 36x36 faces in an image\n",
    "\n",
    "    Args:\n",
    "        img: an image\n",
    "        stride: how far to move between locations at which the detector is run\n",
    "        thresh: probability threshold for calling a detection a face\n",
    "        params: trained face classifier parameters\n",
    "        orientations: the number of HoG gradient orientations to use\n",
    "        wrap180: if true, the HoG orientations cover 180 degrees, else 360\n",
    "\n",
    "    Returns:\n",
    "        outimg: copy of img with face locations marked\n",
    "        probmap: probability map of face detections\n",
    "    \"\"\"\n",
    "    windowsize = 36\n",
    "    if stride > windowsize:\n",
    "        stride = windowsize\n",
    "\n",
    "    height, width = img.shape\n",
    "    probmap = np.zeros([height, width])\n",
    "    outimg = np.array(img)\n",
    "    \n",
    "    new_params = np.zeros([hog_descriptor_size + 1, 1])\n",
    "    new_params[:,0] = params\n",
    "    params = new_params\n",
    "\n",
    "    # Loop over windowsize x windowsize windows, advancing by stride\n",
    "    hog_descriptor_size = 100 * orientations\n",
    "    window_descriptor = np.zeros([1, hog_descriptor_size + 1])\n",
    "\n",
    "    # // slides down then across the image, by stride\n",
    "    for i in range(0, width-windowsize, stride):\n",
    "        for j in range(0, height-windowsize, stride):\n",
    "\n",
    "            # Crop out a windowsize x windowsize window starting at (i,j)\n",
    "            crop = img[i:i+windowsize,j:j+windowsize] \n",
    "\n",
    "            # Compute a HoG descriptor, and run the classifier\n",
    "            window_descriptor[0,0] = 1\n",
    "            window_descriptor[0, 1:] = hog36(crop, orientations, wrap180)\n",
    "            # NEED TO TRAIN AND RUN CLASSIFIER ?? PROB --> FIT () ?? or since trained params good \n",
    "            probability = logistic_prob1(window_descriptor, params) #or need to do both fit + prob \n",
    "\n",
    "            # Mark detection probability in probmap\n",
    "            win_i = i + int((windowsize - stride) / 2)\n",
    "            win_j = j + int((windowsize - stride) / 2)\n",
    "            probmap[win_i:win_i+stride, win_j:win_j+stride] = probability\n",
    "\n",
    "            # If probability of a face is below thresh, continue \n",
    "            # else mark the face on img \n",
    "            if probability < thresh:\n",
    "                continue\n",
    "             \n",
    "            #print(\"got here\")\n",
    "            # Mark the face in outimg\n",
    "            outimg[i, j:j+windowsize] = 255\n",
    "            outimg[i+windowsize-1, j:j+windowsize] = 255\n",
    "            outimg[i:i+windowsize, j] = 255\n",
    "            outimg[i:i+windowsize, j+windowsize-1] = 255\n",
    "\n",
    "    return outimg, probmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'face_classifier.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-edc917f1e529>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfind_faces_single_scale\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfind_faces_single_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'face_data/single_scale_scenes/addams-family.jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msaved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'face_classifier.npz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morientations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrap180\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaved\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaved\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'orientations'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaved\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wrap180'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moutimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_faces_single_scale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morientations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrap180\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cos429\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'face_classifier.npz'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from find_faces_single_scale import find_faces_single_scale\n",
    "img = cv2.imread('face_data/single_scale_scenes/addams-family.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "saved = np.load('face_classifier.npz')\n",
    "params, orientations, wrap180 = saved['params'], saved['orientations'], saved['wrap180']\n",
    "outimg, probmap = find_faces_single_scale(img, 3, 0.95, params, orientations, wrap180)\n",
    "plt.figure(1)\n",
    "plt.title('outimg')\n",
    "plt.imshow(outimg, cmap='gray')\n",
    "plt.show()\n",
    "plt.figure(2)\n",
    "plt.title('probmap')\n",
    "plt.imshow(probmap, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
